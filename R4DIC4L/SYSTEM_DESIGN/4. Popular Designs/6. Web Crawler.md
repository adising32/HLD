---
noteID: b637a096-4bc2-4976-9f6b-a647e5f74a24
---
Watch Hello Interview video

### Goal
Design a web crawler. 
##### FR : 
- Given a set of URLs, download all the web pages addressed by the URLs. 
- Extract URLs from these web pages  
- Add new URLs to the list of URLs to be downloaded. Repeat these 3 steps.
- Ability to crawl and store 1 billion pages.
- Pages with duplicate content should be ignored.
##### NFR
- Scalability: The web is very large. There are billions of web pages out there. Web crawling should be extremely efficient using parallelisation.
- Robustness: The web is full of traps. Bad HTML, unresponsive servers, crashes, malicious links, etc. are all common. The crawler must handle all those edge cases.
- Politeness: The crawler should not make too many requests to a website within a short time interval.
- Extensibility: The system is flexible so that minimal changes are needed to support new content types. For example, if we want to crawl image files in the future, we should not need to redesign the entire system.

#### HLD
![[Screenshot 2024-10-16 at 2.42.06 AM.png]]


**Seed URL** : A web crawler uses seed URLs as a starting point for the crawl process.
**URL Frontier** : The component that stores URLs to be downloaded is called the URL Frontier.
**HTML Downloader** : The HTML downloader downloads web pages from the internet. Those URLs are provided by the URL Frontier.
**Content Parser** : After a web page is downloaded, it must be parsed and validated because malformed web pages could provoke problems and waste storage space.
**Content Storage** : AWS Blob storage is used.
**URL Extractor** : URL Extractor parses and extracts links from HTML pages.
**URL Filter** : The URL filter excludes certain content types, file extensions, error links and URLs in “blacklisted” sites.
**URL Seen?** : “URL Seen?” is a data structure that keeps track of URLs that are visited before or already in the Frontier. “URL Seen?” helps to avoid adding the same URL multiple times as this can increase server load and cause potential infinite loops.

## Deep Dive
#### URL Frontier
A URL frontier is a data structure that stores URLs to be downloaded. The URL frontier is an important component to ensure politeness, URL prioritisation, and freshness.

##### Politeness

![[politeness.png]]
In the above approach each domain has a seperate queue and links from same domain are pushed to corresponding queues. Each worker thread is download URLs from a specific queue.

##### Priority
“Prioritizer” is the component that handles URL prioritisation which can be measured by PageRank, website traffic, update frequency, etc.


![[prioritizer.png]]
Queues with higher priority are selected with higher priority.

##### Freshness
Recrawl based on web pages’ update history. Prioritize URLs and recrawl important pages first and more frequently.

##### Storage
Mostly in memory but periodically flushed to storage.


#### HTML Downloader
The HTML Downloader downloads web pages from the internet using the HTTP protocol.

##### Robots.txt
It specifies what pages crawlers are allowed to download.
Eg
```
User-agent: Googlebot  
Disallow: /creatorhub/*  
Disallow: /rss/people/*/reviews Disallow: /gp/pdp/rss/*/reviews Disallow: /gp/cdp/member-reviews/ Disallow: /gp/aw/cr/
```

##### Performance optimization
- **Distributed crawling** : Crawl jobs are distributed into multiple servers, and each server runs multiple threads.
- **DNS resolver** : Maintaining our DNS cache to avoid calling DNS frequently is an effective technique for speed optimization.
- **Locality** : Distribute crawl servers geographically.
- **Timeout** : If a host does not respond within a predefined time, the crawler will stop the job and crawl some other pages.

#### Robustness
- **Consistent hashing** : This helps to distribute loads among downloaders
- **Save crawl states and data** : To guard against failures, crawl states and data are written to a storage system. A disrupted crawl can be restarted easily by loading saved states and data.

#### Detect and avoid problematic content
- **Redundant content** : Hashes or checksums help to detect duplication
- **Spider traps** : A spider trap is a web page that causes a crawler in an infinite loop. For instance, an infinite deep directory structure. Such spider traps can be avoided by setting a maximal length for URLs.
