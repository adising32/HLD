---
noteID: e080ae3f-ec07-4ef7-8e41-f79b252105fa
---
### Relational Model vs Document Model
 
| Relational                                                                                                                                                    | Document                                                                                                                                                                                                           |
| ------------------------------------------------------------------------------------------------------------------------------------------------------------- | ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ |
| Data is organized into tables (relations) with rows and columns.                                                                                              | Data is stored in documents, typically in JSON, BSON, or XML format.                                                                                                                                               |
| Requires a fixed schema defined before data entry.<br>**Schema on write** : the schema is explicit and the database ensures all written data con‐ forms to it | Schema-less or dynamic schema, allowing for easy changes to the data structure without major migrations.<br>**Schema on read** : the structure of the data is implicit, and only interpreted when the data is read |
| Enforces strong data integrity through constraints (e.g., primary keys, foreign keys).                                                                        | Typically offers less stringent data integrity than relational databases.                                                                                                                                          |
| Uses SQL (Structured Query Language) for querying and manipulating data.                                                                                      | Uses various query languages, often specific to the database system (e.g., MongoDB uses its own query language).                                                                                                   |
| Best suited for applications requiring complex transactions and strong consistency (e.g., banking systems, enterprise applications).                          | Ideal for applications with unstructured or semi-structured data, such as content management systems, real-time analytics, and web applications.                                                                   |
| Better support for one-to-many and many-to-many relations and joins                                                                                           | Weaker support but more closer to data structure used by application and better performance based on locality                                                                                                      |

### Query Languages

##### Imperative 
An imperative language tells the computer to perform certain operations in a certain order
##### Declartive
In a declarative query language, you just specify the pattern of the data you want—what conditions the results must meet, and how you want the data to be transformed (e.g., sorted, grouped, and aggregated)—but not how to achieve that goal.

### Simple Database
Assume a simple database which stores key, value in a file. 
* Put(k,v) operation appends to log file
* Get(k) operation returns the most recent value.
##### Compaction
Appending to same file will make the file very large in size. Instead a new file is created after threshold size is reached called segments. In background the individual files made smaller by removing duplicate entries. This is called compaction. Later these small files can also be merged. The merging and compaction of frozen segments can be done in a background thread, and while it is going on, we can still continue to serve read and write requests as normal, using the old segment files.
##### Deletion
Append a special deletion record to the data file (sometimes called a tombstone). When log segments are merged, the tombstone tells the merging process to discard any previous values for the deleted key.
##### Crash Recovery
In case of failures in memory hash indexes are lost. This needs to be regenerated by reading the entire log file.
##### Concurrency control
As writes are appended to the log in a strictly sequential order, a common imple‐ mentation choice is to have only one writer thread. Data file segments are append-only and otherwise immutable, so they can be read concurrently by mul‐ tiple threads.


#### Indexing
##### Hash Indexing
In our simple database, reads are slower. Hash index stores offset value in file for each key. So while reading it directly seeks to particular offset in file. This has a tradeoff. Writes become slower as with each write its index also needs to be updated. Each segment has its own in-memory hash table, mapping keys to file offsets. In order to find the value for a key, we first check the most recent segment’s hash map; if the key is not present we check the second-most-recent segment, and so on.
* Limitations
	* The hash table must fit in memory
	* Range queries are not efficient.

###### SST (Sorted String Tables)
In this method data which is stored in segments should be sorted by key. This is achieved by first writing the data to an in memory **Balanced Binary Search Tree** (Red Black tree or AVL tree). This data structure can store data in sorted order. This in-memory tree is called a **memtable**. Once a threshold of data is entered into the tree, it is flushed to disk. Compaction and merging can run in background over these segments. For crash recovery, we can keep a separate log on disk to which every write is immediately appended and later build up the in memory tree from the log file. Now since the data is stored in sorted order, we get some 
* Advantages
	* Merging segments is simple and efficient. Use merge sort.
	* Hash indexes of all keys may not be kept in memory. Hence indexing can be **sparse**. Say we are looking for key 'abc', and we donot have its hash index in memory. But we have hash indexes for 'aba' and 'abg'. So the offset of 'abc' should be between these indexes as segments are sorted.
	* 2. Since read requests need to scan over several key-value pairs in the requested range anyway, it is possible to group those records into a block and compress it before writing it to disk. Each entry of the sparse in-memory index then points at the start of a compressed block. Besides saving disk space, compression also reduces the I/O bandwidth use.
* Disadvantages
	* Algorithm can be slow when looking up keys that do not exist in the database as we need to check memtable, then all segments. **Bloom Filters** can be used to mitigate this to some extent.

##### B-Trees
B-trees break the database down into fixed-size blocks or pages, traditionally 4 KB in size, and read or write one page at a time. One page is designated as the root of the B-tree. The page contains several keys and references to child pages. Each child is responsible for a continuous range of keys, and the keys between the references indicate where the boundaries between those ranges lie.
Crash recovery : An additional data structure on disk: a write-ahead log (WAL, also known as a redo log). This is an append-only file to which every B-tree modification must be written before it can be applied to the pages of the tree itself. When the data‐ base comes back up after a crash, this log is used to restore the B-tree back to a con‐ sistent state.
###### Clustered Index 
Table data is stored along with index
###### Heap file
Table data is stored in a separate place callled heap file and index contains only references to positions in heap file.
###### Covering index
Stores some of a table’s col‐ umns within the index.
###### Multi column index
The most common type of multi-column index is called a concatenated index, which simply combines several fields into one key by appending one column to another.


##### Full-text search and fuzzy indexes ??


#### Data Encoding

The translation from the in-memory representation to a byte sequence is called encoding (also known as serialization or marshalling), and the reverse is called decoding (parsing, deserialization, unmarshalling).


### Scaling

#### Vertical Scaling 
Vertical scaling, also known as scaling up, is the scaling by adding more power (CPU, RAM, DISK, etc.) to an existing machine.
> **Limitations**\
	There are hardware limits.
	Greater risk of single point of failures.
	The overall cost of vertical scaling is high


#### Horizontal Scaling
Horizontal scaling, also known as ***Sharding***, is the practice of adding more servers. Sharding separates large databases into smaller, more easily managed parts called shards. Each shard shares the same schema, though the actual data on each shard is unique to the shard. 
The most important factor to consider when implementing a sharding strategy is the choice of the ***sharding key***. Sharding key (known as a partition key) consists of one or more columns that determine how data is distributed. A sharding key allows you to retrieve and modify data efficiently by routing database queries to the correct database.

**Challenges**
* Resharding data is needed when 
	-  a single shard could no longer hold more data due to rapid growth.
	-  Certain shards might experience shard exhaustion faster than others due to uneven data distribution.
* Excessive access to a specific shard could cause server overload. This problem is also known as ***Celebrity Problem or Hotspot key problem***.
* Once a database has been sharded across multiple servers, it is hard to perform join operations across database shards. A common workaround is to de- normalize the database so that queries can be performed in a single table.

